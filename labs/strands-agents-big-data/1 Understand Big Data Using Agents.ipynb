{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Agents and Big Data Analytics - Better Together\n",
    "Up to 80% of the data that enterprises generate is unstructured and semi-structured data. Your AI Agents will provide richer insights by querying TBs of data stored in S3 data lake.<br><br>\n",
    "Do AI developers need to learn data engineering skills such as distributed spark processing, metadata catalogs, complex distributed SQL queries, NoSQL queries etc to query TBs and PBs of data in data lakes?<br><br>\n",
    "Fortunately, the answer is no. You just need to know the basics. You can use Strands Agents, built-in strands tools such as use_aws, and MCP server such as AWS Data Processing MCP Server to accomplish what most data engineers can, while writing few lines of code.<br><br>\n",
    "This notebook will help you bridge the gap between a data engineer who knows how to use AWS Glue, Amazon Athena with an AI engineer who knows how to use Strands SDK, Amazon Bedrock. You will be able to do sophisticated analytics with natural language questions.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOAL - Perform big data analytics using AI Agents without writing big data analytics code. <br>\n",
    "The AI Agent will use built-in strands tools and MCP server to get the job done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this Notebook\n",
    "You will learn how to use AI Agents to discover metadata of big-data stored in parquet files using Agent tools and MCP server that in turn uses AWS Glue and Amazon Athena.<br><br>\n",
    "We will use NYC Taxi and Limousine Commission (TLC) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "Install required dependencies for the notebook including Strands SDK, AWS SDK, and MCP client libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages from requirements.txt\n",
    "!pip install -r requirements.txt --quiet\n",
    "!pip install --upgrade strands-agents uv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab configurations\n",
    "STACK_NAME = \"AwsLabBigDataAgentStack\"\n",
    "\n",
    "# Region\n",
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Setup the logging\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%m/%d %H:%M:%S',\n",
    "    filename='strands_debug.log'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Requisites (TBD)\n",
    "1/ Create an S3 bucket or reuse an existing one.<br>\n",
    "2/ Create a folder in the S3 bucket so you can upload Parquet files to this folder.<br>\n",
    "3/ Download the data from NYC Taxi and Limousine Commission (TLC) https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page <br>Expand 2025 and download Yellow Taxi ip Records.<br>\n",
    "4/ Upload them to the folder in S3 bucket.<br><br>\n",
    "5/ Specify the following in inputs.txt file in the current folder<br>\n",
    "---- name of the s3 bucket<br>\n",
    "---- name of the folder where the parquet files files are uploaded. Ex: s3://my-folder-name/nyc_data_source/<br>\n",
    "---- name of the aws region. Ex: us-west-2<br>\n",
    "---- name of the aws role that has permissions for AWS Glue, Amazon Athena, Amazon S3, Amazon Bedrock, Amazon Knowledge Bases<br>\n",
    "---- name of the S3 folder where Amazon Athena should store the results. Ex: s3://my-folder-name/athena_results/<br>\n",
    "---- name of the S3 folder where Athena results should be stored<br><br>\n",
    "\n",
    "6/ You also need run aws configure on your laptop and provide access key ID, key access, default region name<br>\n",
    "7/ And lastly you need model access to all Anthropic Claude Sonnet Models and Amazon Titan Embeddings model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.deploy_cfn import deploy_infrastructure\n",
    "deploy_infrastructure(STACK_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will do:\n",
    "# 1. Download files from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "# 2. Upload the files to a S3 bucket\n",
    "from utils.s3_big_data_setup import prepare_data\n",
    "\n",
    "file_urls = [ # These are trip records of NYC Taxi.\n",
    "        \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-05.parquet\",\n",
    "        \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-06.parquet\",\n",
    "        \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2025-05.parquet\",\n",
    "        \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2025-06.parquet\"\n",
    "    ]\n",
    "\n",
    "prepare_data(stack_name=STACK_NAME, suffix=\"workshop\", file_urls=file_urls, table_name=\"nyc_taxi_trip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies and AWS Configuration\n",
    "Import required libraries and configure AWS settings for the data processing workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os, time, boto3, json\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "from strands_tools import use_aws, file_write, file_read, file_write, sleep, python_repl\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "\n",
    "# Bypass tool consent for automated execution\n",
    "os.environ[\"BYPASS_TOOL_CONSENT\"] = \"true\"\n",
    "# Specify that if python_repl tool is used, it shouldnt wait for user interaction\n",
    "os.environ[\"PYTHON_REPL_INTERACTIVE\"] = \"False\"\n",
    "\n",
    "model_list = ['deepseek.v3-v1:0', 'qwen.qwen3-coder-30b-a3b-v1:0', 'us.anthropic.claude-3-7-sonnet-20250219-v1:0', 'us.anthropic.claude-sonnet-4-20250514-v1:0', 'openai.gpt-oss-20b-1:0', 'openai.gpt-oss-120b-1:0']\n",
    "\n",
    "# We will use the following model in Strands Agent\n",
    "model_id = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to discover the metadata and catalog it. <br><br>\n",
    "AWS Glue can help crawl the data, extract the metadata, and create a catalog using AWS Glue Catalog.<br><br>\n",
    "Let's NOT write code to create AWS Glue jobs. Instead, let's write a SYSTEM PROMPT that will instruct an AI Agent to do this job for us.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catalog Data in S3 as Glue Catalog Database and Tables\n",
    "Create a Strands Agent that uses use_aws strands tool to create AWS Glue crawlers and catalog the S3 data automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_big_data import load_system_prompt_from_file\n",
    "# Let's load the system prompt from file\n",
    "crawl_task = load_system_prompt_from_file(\"crawl_task_prompt.txt\", stack_name = STACK_NAME)\n",
    "crawl_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Claude 3.7 Sonnet model via Bedrock\n",
    "model = BedrockModel(model_id=model_id, temperature=0.1)  # Low temperature for consistent structured output\n",
    "\n",
    "# Create the Strands Agent. \n",
    "# use_aws tool will create a glue crawler and create glue catalog db and table schema\n",
    "# sleep tool will help the agent wait until the glue crwler job is finished.\n",
    "# This task might take a few minutes. You can go to AWS Console and see the Glue Crawler jobs that are being created.\n",
    "crawl_agent = Agent(model=model, tools=[use_aws, sleep])\n",
    "\n",
    "crawl_response = crawl_agent(crawl_task)\n",
    "print(crawl_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_big_data import print_tokens_costs\n",
    "\n",
    "# Let's print the token costs\n",
    "print_tokens_costs(crawl_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Glue Database and Tables Names\n",
    "Parse the agent response to extract the Glue database name and Table names for use in subsequent queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strands Agenst allows you to extract information in a structured dictionary output.\n",
    "\n",
    "# Define the structured template\n",
    "class GlueDbTableInfo(BaseModel):\n",
    "    catalog_db_name: str = Field(description=\"Name of the Glue Catalog Database\")\n",
    "    catalog_table_names: List[str] = Field(description=\"List of glue table names\")\n",
    "\n",
    "# Use the structured_output method\n",
    "glue_db_table_info = crawl_agent.structured_output(GlueDbTableInfo, f\"Extract Glue Catalog Database name and the list of glue table names\")\n",
    "print(glue_db_table_info)\n",
    "catalog_db_name = glue_db_table_info.catalog_db_name\n",
    "catalog_table_names = glue_db_table_info.catalog_table_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Labs has created many MCP servers for AI Agent developers to consume. One of them is AWS data processing MCP server: https://awslabs.github.io/mcp/servers/aws-dataprocessing-mcp-server <br><br>\n",
    "This MCP server exposes AI Agent tools to do operations on big data using AWS Glue, Amazon EMR, and Amazon Athena.<br><br>\n",
    "The best part is that we don't have to know the intricacies of these services other than high level basics.<br><br>\n",
    "Let's create an MCP client for this MCP server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP Client Setup\n",
    "We wll get tools exposed by an MCP server to discover partition columns and keys from the data in S3.<br><br>\n",
    "Initialize the AWS Data Processing MCP server client to provide AI agents with AWS Glue, EMR, and Athena capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MCP client libraries\n",
    "from mcp import stdio_client, StdioServerParameters\n",
    "from strands.tools.mcp import MCPClient\n",
    "import boto3\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "# Create MCP client for AWS data processing server\n",
    "# This provides tools for Glue, EMR, and Athena operations\n",
    "data_mcp_client = MCPClient(lambda: stdio_client(\n",
    "    StdioServerParameters(\n",
    "        command=\"uvx\",  # Use uvx to run the MCP server\n",
    "        args= [\n",
    "            \"awslabs.aws-dataprocessing-mcp-server@latest\",\n",
    "            \"--allow-write\",  # Enable write operations\n",
    "        ],\n",
    "        env= {\n",
    "            \"AWS_ACCESS_KEY_ID\": credentials.access_key,\n",
    "            \"AWS_SECRET_ACCESS_KEY\": credentials.secret_key,\n",
    "            \"AWS_SESSION_TOKEN\": credentials.token,\n",
    "            \"FASTMCP_LOG_LEVEL\": \"ERROR\",  # Minimize logging noise\n",
    "            \"AWS_REGION\": session.region_name      # Set AWS region\n",
    "      }\n",
    "    )\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import boto3\n",
    "\n",
    "sts_client = boto3.client('sts')\n",
    "response = sts_client.get_caller_identity()\n",
    "\n",
    "arn = response['Arn']\n",
    "# Example: arn:aws:sts::123456789012:assumed-role/MyRole/MySession\n",
    "if \"assumed-role\" in arn:\n",
    "    role_name = arn.split('/')[-2]\n",
    "    print(f\"IAM Role Name: {role_name}\")\n",
    "else:\n",
    "    print(\"Not an assumed role session.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the tools offered by the MCP Server\n",
    "This will help you understand what the tools do. This is extracted by looking at doc strings or tool spec of the tools.<br><br>\n",
    "Agents send the doc string or the tool spec of the tools to the LLM along with a task.<br>\n",
    "This helps LLM to reason and decide which tool to use for which task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MCP client and execute task\n",
    "import json\n",
    "\n",
    "tools_char_count = 0\n",
    "tool_count = 0\n",
    "with data_mcp_client:\n",
    "    # Get available tools from the MCP server\n",
    "    data_tools = data_mcp_client.list_tools_sync()\n",
    "    # first_tool = data_tools[0]\n",
    "    # print(dir(first_tool))    \n",
    "    # Iterate through each tool\n",
    "    for tool in data_tools:\n",
    "        print(f\"Tool: {tool.tool_name}\")\n",
    "        if hasattr(tool, 'tool_spec'):            \n",
    "            print(f\"Tool: {tool.tool_spec['description']}\")\n",
    "            # print(json.dumps(tool.tool_spec, indent=2)) # uncomment this to see function parameters and what they mean   \n",
    "            tools_char_count += len(json.dumps(tool.tool_spec))\n",
    "            tool_count += 1\n",
    "        print(\"-\" * 50)\n",
    "print(f\"The number of characters in the spec of all the {tool_count} tools = {tools_char_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the content that the tables have and what the columns mean. \n",
    "Store this in a file and pass it to the agent. This will help agent construct SQL queries properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the definition of the table and columns and store it in a file.\n",
    "table_def_system_prompt = f\"\"\"\n",
    "You are an expert AWS data analyst assistant specializing in querying data stored in S3 data lakes using AWS Glue and Amazon Athena.\n",
    "\n",
    "## IMPORTANT: \n",
    "- For any long running jobs, sleep for 20 seconds and check status until the job is finished. Do not give up until the job actually finishes.\n",
    "\"\"\"\n",
    "query_in = f\"\"\"Query the database {catalog_db_name}, identify the tables in it, identify the columns and their types. Gather more business context of what the database, tables, and the columns are storing by sampling a few rows of data. Then store the database name, table names, their purpose; and column name and its type and purpose in a json file named metadata.json.\"\"\"\n",
    "\n",
    "column_def_response = \"\"\n",
    "# Get the data processing tools from MCP server\n",
    "with data_mcp_client:\n",
    "    data_tools = data_mcp_client.list_tools_sync()\n",
    "    curated_data_tools = ['manage_aws_athena_query_executions', 'manage_aws_glue_tables']\n",
    "\n",
    "    # Extract just the tools that we need.\n",
    "    filtered_tools = [tool for tool in data_tools if tool.tool_name in curated_data_tools]\n",
    "    filtered_tools += [file_write, file_read, sleep]\n",
    "\n",
    "    # Pass the system prompt, the LLM we use with bedrock, and all the tools to the agent\n",
    "    column_def = Agent(system_prompt = table_def_system_prompt, model=model, tools=filtered_tools)\n",
    "    # Invoke the agent with each of the query\n",
    "    column_def_response = column_def(query_in)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thalaiva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
