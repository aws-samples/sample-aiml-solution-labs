{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOAL - Query big data using AI Agents without writing big data analytics code. <br>\n",
    "The AI Agent will use built-in strands tools and MCP server to get the job done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "Install required dependencies for the notebook including Strands SDK, AWS SDK, and MCP client libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages from requirements.txt\n",
    "!pip install -r requirements.txt --quiet\n",
    "!pip install --upgrade strands-agents --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%m/%d %H:%M:%S',\n",
    "    filename='strands_debug.log'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Requisites\n",
    "You have run the Notebook 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies and AWS Configuration\n",
    "Import required libraries and configure AWS settings for the data processing workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os, time, boto3, json\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "from strands_tools import use_aws, file_write, file_read, file_write, sleep, python_repl\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "\n",
    "# Bypass tool consent for automated execution\n",
    "os.environ[\"BYPASS_TOOL_CONSENT\"] = \"true\"\n",
    "# Specify that if python_repl tool is used, it shouldnt wait for user interaction\n",
    "os.environ[\"PYTHON_REPL_INTERACTIVE\"] = \"False\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Metadata Info from the json file that we created in the previous notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file metadata.json into a dictionary without using file_read\n",
    "with open('metadata.json', 'r') as f:\n",
    "    db_metadata = json.load(f)\n",
    "db_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP Client Setup\n",
    "We wll get tools exposed by an MCP server to discover partition columns and keys from the data in S3.<br><br>\n",
    "Initialize the AWS Data Processing MCP server client to provide AI agents with AWS Glue, EMR, and Athena capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MCP client libraries\n",
    "from mcp import stdio_client, StdioServerParameters\n",
    "from strands.tools.mcp import MCPClient\n",
    "\n",
    "# Create MCP client for AWS data processing server\n",
    "# This provides tools for Glue, EMR, and Athena operations\n",
    "import boto3\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "# Create MCP client for AWS data processing server\n",
    "# This provides tools for Glue, EMR, and Athena operations\n",
    "data_mcp_client = MCPClient(lambda: stdio_client(\n",
    "    StdioServerParameters(\n",
    "        command=\"uvx\",  # Use uvx to run the MCP server\n",
    "        args= [\n",
    "            \"awslabs.aws-dataprocessing-mcp-server@latest\",\n",
    "            \"--allow-write\",  # Enable write operations\n",
    "        ],\n",
    "        env= {\n",
    "            \"AWS_ACCESS_KEY_ID\": credentials.access_key,\n",
    "            \"AWS_SECRET_ACCESS_KEY\": credentials.secret_key,\n",
    "            \"AWS_SESSION_TOKEN\": credentials.token,\n",
    "            \"FASTMCP_LOG_LEVEL\": \"ERROR\",  # Minimize logging noise\n",
    "            \"AWS_REGION\": session.region_name      # Set AWS region\n",
    "      }\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Ask Natural Language Questions to AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_big_data import print_tokens_costs, load_system_prompt_from_file\n",
    "\n",
    "# Let's load the system prompt from file for running queries on data in S3 data lake\n",
    "query_system_prompt = load_system_prompt_from_file(\"text_to_sql_prompt.txt\", db_metadata=db_metadata)\n",
    "query_system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['deepseek.v3-v1:0', 'qwen.qwen3-coder-30b-a3b-v1:0', 'us.anthropic.claude-3-7-sonnet-20250219-v1:0', 'us.anthropic.claude-sonnet-4-20250514-v1:0', 'openai.gpt-oss-20b-1:0', 'openai.gpt-oss-120b-1:0']\n",
    "\n",
    "# We will use the following model in Strands Agent\n",
    "model_id = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "\n",
    "# Let's create a reusable function to process a query and return a response in a structured dictionary format\n",
    "def  get_query_response(query, model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\"):\n",
    "\n",
    "    # We want the response to be in structured dictionary format that returns SQL statement, its reasoning and the final response.\n",
    "    class SQLQuery(BaseModel):\n",
    "        sql_statement: str = Field(description=\"The SQL query that was generated\")\n",
    "        reasoning: str = Field(description=\"Step by step explanation of how the natural language question was translated to this SQL statement\")\n",
    "\n",
    "    class QueryResponse(BaseModel):        \n",
    "        sql_queries: List[SQLQuery] = Field(description=\"List of SQL queries with their reasoning\")\n",
    "        final_response: str = Field(description=\"The final response generated\")\n",
    "\n",
    "    # Cerate the Bedrock Model using model_id\n",
    "    model = BedrockModel(model_id=model_id)\n",
    "\n",
    "    #Let's us ethe MCP client we created earlier\n",
    "    with data_mcp_client:\n",
    "        # Get the data processing tools from MCP server\n",
    "        data_tools = data_mcp_client.list_tools_sync()\n",
    "\n",
    "        # Optimize tools by passing just what we need instead of all 32 tools\n",
    "        curated_data_tools = ['manage_aws_athena_query_executions']\n",
    "\n",
    "        # Extract just the tools that we need.\n",
    "        filtered_tools = [tool for tool in data_tools if tool.tool_name in curated_data_tools]\n",
    "\n",
    "        #Add the following tools so we can generate charts or read / write to files if needed.\n",
    "        final_tools = [python_repl, file_read, file_write] + filtered_tools\n",
    "    \n",
    "        # Pass the system prompt, the LLM we use with bedrock, and all the tools to the agent\n",
    "        data_lake_agent = Agent(system_prompt = query_system_prompt, model=model, tools=final_tools)\n",
    "\n",
    "        # Invoke the Agent\n",
    "        temp_response = data_lake_agent(query)\n",
    "\n",
    "        #Convert the agents response into a structured output\n",
    "        response = data_lake_agent.structured_output(QueryResponse, \"Extract the structured output of sql queries, reasoning, and the final response\")\n",
    "\n",
    "        #Convert the object into a dictionary\n",
    "        response_dict = response.model_dump()\n",
    "        return response_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_query_response(f\"How many rides went to Airport each month in 2025?\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_query_response(f\"How many taxi vendors are there? Plot a bar chart with ride count and fare amount.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_query_response(\"For the top 25 percentile of ride fares per yellow / green class, what is ratio of tips to total fare? If I am a taxi driver which routes and times should I drive to get the most tips?\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thalaiva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
